<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Read Word2vec Code and Some Detail - welcome to lc&#39;s blogs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="lc" /><meta name="description" content="朋友发了一个有趣的[链接](https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&mid=2247485614&idx=1&sn=6ff3dc634cc8a1f354695fd4e139b0b3&chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&mpshare=1&scene=24&srcid=0607FNjJEeaXpNxLNtpWugS2&pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd)过来，关于word2vec的，然后就有了这篇文章" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.56.0-DEV with even 4.0.0" />


<link rel="canonical" href="http://awsomename.github.io/post/read-word2vec-code-and-some-detail/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Read Word2vec Code and Some Detail" />
<meta property="og:description" content="朋友发了一个有趣的[链接](https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485614&amp;idx=1&amp;sn=6ff3dc634cc8a1f354695fd4e139b0b3&amp;chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&amp;mpshare=1&amp;scene=24&amp;srcid=0607FNjJEeaXpNxLNtpWugS2&amp;pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd)过来，关于word2vec的，然后就有了这篇文章" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://awsomename.github.io/post/read-word2vec-code-and-some-detail/" />
<meta property="article:published_time" content="2019-06-09T18:34:33+08:00" />
<meta property="article:modified_time" content="2019-06-09T18:34:33+08:00" />
<meta itemprop="name" content="Read Word2vec Code and Some Detail">
<meta itemprop="description" content="朋友发了一个有趣的[链接](https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485614&amp;idx=1&amp;sn=6ff3dc634cc8a1f354695fd4e139b0b3&amp;chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&amp;mpshare=1&amp;scene=24&amp;srcid=0607FNjJEeaXpNxLNtpWugS2&amp;pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd)过来，关于word2vec的，然后就有了这篇文章">


<meta itemprop="datePublished" content="2019-06-09T18:34:33&#43;08:00" />
<meta itemprop="dateModified" content="2019-06-09T18:34:33&#43;08:00" />
<meta itemprop="wordCount" content="2297">



<meta itemprop="keywords" content="word2vec,NLP," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Read Word2vec Code and Some Detail"/>
<meta name="twitter:description" content="朋友发了一个有趣的[链接](https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485614&amp;idx=1&amp;sn=6ff3dc634cc8a1f354695fd4e139b0b3&amp;chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&amp;mpshare=1&amp;scene=24&amp;srcid=0607FNjJEeaXpNxLNtpWugS2&amp;pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd)过来，关于word2vec的，然后就有了这篇文章"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Lc&#39;s Blogs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Lc&#39;s Blogs</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Read Word2vec Code and Some Detail</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-06-09 </span>
        <div class="post-category">
            <a href="/categories/nlp/"> NLP </a>
            </div>
          <span class="more-meta"> 2297 words </span>
          <span class="more-meta"> 5 mins read </span>
        
      </div>
    </header>

    
    <div class="post-content">
      <p>朋友(Q God)发了一个有趣的<a href="https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485614&amp;idx=1&amp;sn=6ff3dc634cc8a1f354695fd4e139b0b3&amp;chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&amp;mpshare=1&amp;scene=24&amp;srcid=0607FNjJEeaXpNxLNtpWugS2&amp;pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd">链接</a>过来，关于word2vec的，然后就有了这篇文章</p>

<p>本文讨论了关于word2vec的经典解释：Skip-Gram模型，负采样训练。</p>

<p>本文中，上文主要指朋友发的<a href="https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485614&amp;idx=1&amp;sn=6ff3dc634cc8a1f354695fd4e139b0b3&amp;chksm=fd16f9b1ca6170a78e721df8d34fce70303aa67ee0d2bfb498b4d0015a2dffaa939bbe077ff1&amp;mpshare=1&amp;scene=24&amp;srcid=0607FNjJEeaXpNxLNtpWugS2&amp;pass_ticket=V5RKkOY%2BgoyS%2Bmyz2ZPNVWpSgMlbqy89xa6maVii56%2BWngb%2FbfH6eFjtcPUbLAyB#rd">链接</a></p>

<p>本文中，w2v就是指SkipGram负采样的训练方式。</p>

<p>本文的<a href="(https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c)">代码</a>地址
那么就开始吧！！</p>

<p>(这也是本人MarkDown写的第一篇技术博客，如有问题请多指教)</p>

<h1 id="上文中的观点和问题">上文中的观点和问题</h1>

<p>上文指出，大部分的博客关于w2c的解释如下所示：</p>

<p><img src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBKLuofqf0zXxr8VV8yRMF5IJs92SekN5KTfYqxy7gkZzMnJa8ZbpmFibRUaF1tCeibPHPh4uYB8c5A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="" />
但是上文中说，上图只能看出有两个向量，而w2c中的实现和这个完全不同</p>

<p>实际的实现中，每个单词有两个向量，一个是作为中心词的向量（也是最终作为词向量的向量），一个是作为上下文时的向量。</p>

<p>分别保存在两个矩阵（也就是数组中）。</p>

<p>而且这里使用了特殊的初始化技巧，对于中心词向量，是随机初始化的。对于上下文词向量，是使用零初始化的。</p>

<p>貌似在word2vec的论文中，并没有提到这一点。所以上文作者提出了他的质疑。</p>

<p>上文的编辑贴出作者的观点：
&gt;   因为负样本 (Negative Sample) 来自全文上下，并没太根据词频来定权重，这样选哪个单词都可以，通常这个词的向量还没经过多少训练。
&gt;   而如果这个向量已经有了一个值，那么它就可以随意移动 (Move Randomly) 中心词了。
&gt;   解决方法是，把所有负样本设为零，这样依赖只有那些比较高频出现的向量，才会影响到另外一个向量的表征。</p>

<p>原作者的观点<a href="https://github.com/bollu/bollu.github.io">地址</a></p>

<hr />

<h1 id="实地考察的w2c的代码实现">实地考察的w2c的代码实现</h1>

<p>通过上文，和实际考察，可以确定：
&gt;   syn0数组，存储每个词作为中心词的向量，也是最终的词向量。是随机初始化的</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></pre></td>
<td class="lntd">
<pre class="chroma">https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369
    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) {
        next_random = next_random * (unsigned long long)25214903917 + 11;
        syn0[a * layer1_size + b] = 
        (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;
    }</pre></td></tr></table>
</div>
</div>
<blockquote>
<p>syn1neg数组，存储每个词作为上下文词的向量，是零初始化的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365
    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++)
        syn1neg[a * layer1_size + b] = 0;</pre></td></tr></table>
</div>
</div></blockquote>

<p>核心的训练函数是：
<code>TrainModelThread(void *id)</code></p>

<p>其中的核心代码是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></pre></td>
<td class="lntd">
<pre class="chroma">if (negative &gt; 0) for (d = 0; d &lt; negative + 1; d++) {
    if (d == 0) {
        target = word;
        label = 1;
    } else {
        next_random = next_random * (unsigned long long)25214903917 + 11;
        target = table[(next_random &gt;&gt; 16) % table_size];
        if (target == 0) target = next_random % (vocab_size - 1) + 1;
        if (target == word) continue;
        label = 0;
    }
    l2 = target * layer1_size;
    f = 0;
    for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];
    if (f &gt; MAX_EXP) g = (label - 1) * alpha;
    else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;
    else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
    for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
    for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];
}
// Learn weights input -&gt; hidden
for (c = 0; c &lt; layer1_size; c++) syn0[c + l1] += neu1e[c];</pre></td></tr></table>
</div>
</div>
<p>这段代码核心功能是，针对某个中心词，做基于负采样的计算，累积的梯度会更新到syn0，也就是作为中心词的数组。</p>

<p>可以看出，确实是用了两个数组，而且syn1neg的更新并非是从syn0中直接抽取，是自己逐步更新的。
（在看代码之前，我猜测，上下文的数组只是大部分为零，从中心词数组中抽取高频词的向量更新自己，这里看来我猜测的不对，打脸了）
这里基本验证了(虽然上文标题起的些许夸张)，原作者的观点是对的。有些细节的问题，后面讨论。</p>

<hr />

<h1 id="本文观点和私货">本文观点和私货</h1>

<p>这确实是一个问题。这可能算不上是学术造假，但是这个问题确实是会给尝试复现的人造成困扰，困惑。
原作者的分析也是很到位。但是有一点，就是<strong>“没太根据词频来定位权重”</strong>，这一点我有些疑虑。
w2c中是根据<code>table</code>来抽样的，这个数据结构的生成可以看到是根据词频生成的，代码如下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></pre></td>
<td class="lntd">
<pre class="chroma">void InitUnigramTable() {
  int a, i;
  double train_words_pow = 0;
  double d1, power = 0.75;
  table = (int *)malloc(table_size * sizeof(int));
  for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);
  i = 0;
  d1 = pow(vocab[i].cn, power) / train_words_pow;
  for (a = 0; a &lt; table_size; a++) {
    table[a] = i;
    if (a / (double)table_size &gt; d1) {
      i++;
      d1 += pow(vocab[i].cn, power) / train_words_pow;
    }
    if (i &gt;= vocab_size) i = vocab_size - 1;
  }
}</pre></td></tr></table>
</div>
</div>
<p>可以看出，词频越高的词，在table中占的坑位越多，抽中的概率就越大。</p>

<h2 id="其他的发现-关于skipgram模式的理解">其他的发现，关于skipgram模式的理解</h2>

<p>最早我对skipgram的理解，是中心词f(focus)，对应着上下文的词c(content)，f对每个c是正例，f对其他词是负例。</p>

<p>而在这个w2v中，实现是这样的，对中心词f，当前上下文中的c，c对f是正例，c对其他是负例。代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></pre></td>
<td class="lntd">
<pre class="chroma">void *TrainModelThread(void *id) {
    ...
    while (1) {
        ...
        if (cbow) {  //train the cbow architecture
            ...
        } else {  //train skip-gram
            for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) {
                c = sentence_position - window + a;
                if (c &lt; 0) continue;
                if (c &gt;= sentence_length) continue;
                last_word = sen[c];
                if (last_word == -1) continue;
                l1 = last_word * layer1_size;
                for (c = 0; c &lt; layer1_size; c++) neu1e[c] = 0;
                // HIERARCHICAL SOFTMAX
                if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) {
                    ...
                }
                // NEGATIVE SAMPLING
                if (negative &gt; 0) for (d = 0; d &lt; negative + 1; d++) {
                  if (d == 0) {
                    target = word;
                    label = 1;
                  } else {
                    next_random = next_random * (unsigned long long)25214903917 + 11;
                    target = table[(next_random &gt;&gt; 16) % table_size];
                    if (target == 0) target = next_random % (vocab_size - 1) + 1;
                    if (target == word) continue;
                    label = 0;
                  }
                  l2 = target * layer1_size;
                  f = 0;
                  for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];
                  if (f &gt; MAX_EXP) g = (label - 1) * alpha;
                  else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;
                  else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
                  for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
                  for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];
                }
                // Learn weights input -&gt; hidden
                for (c = 0; c &lt; layer1_size; c++) syn0[c + l1] += neu1e[c];
            }
        }
        ...
    }
  ...
}</pre></td></tr></table>
</div>
</div>
<p>以上是关键代码，简单的讲，word是当前句子中的中心词（词f），last_word是当前的上下文词（词c），负采样采到的词是target。
l1是last_word的词向量坐标，l2是target负采样的词的向量坐标，训练中，是基于这两个坐标指示的向量，计算梯度和更新上下文词向量（syn1neg数组)
所以这里，正样例是c到f，负样例是c到其他。和我理解的是相反的。</p>

<p>当然这从原理上讲，貌似无伤大雅，因为我理解的word2vec的假设是，如果两个词很像，它会有相似的上下文。上下文的词和中心词的对应关系本身也没有方向性。不过还是一个很有意思的问题。</p>

<p>所以下一步我会看一下tensorflow的nec_loss是怎样实现的，对比一下。
近期我也会看一下w2c的原论文，证实以上种种疑惑，再来补充。</p>

<p>如果有任何问题请和我指出，感谢您的指证。
可以在我的<a href="http://blog.tureornot.com/2019/06/09/关于word2wec的一点讨论">wordpress</a>留言。</p>
    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/word2vec/">word2vec</a>
          <a href="/tags/nlp/">NLP</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/make_github_pages/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Make_github_pages</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/meet_google/">
            <span class="next-text nav-default">Meet_google</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:liuchang.hit.cs@email.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/AwsomeName" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/lcself/posts" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://blog.tureornot.com" class="iconfont icon-gitlab" title="gitlab"></a>
  <a href="http://awsomename.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">lc</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
